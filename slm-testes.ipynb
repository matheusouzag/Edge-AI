{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.2:1b', modified_at=datetime.datetime(2025, 2, 19, 17, 25, 40, 881939, tzinfo=TzInfo(-03:00)), digest='baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878', size=1321098329, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q8_0')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2025, 2, 19, 17, 21, 7, 977010, tzinfo=TzInfo(-03:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do Modelo e Teste de Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.2:1b' created_at='2025-03-13T17:42:47.7084715Z' done=True done_reason='stop' total_duration=5539699400 load_duration=3957419600 prompt_eval_count=32 prompt_eval_duration=1035000000 eval_count=8 eval_duration=523000000 response='The capital of France is Paris.' context=[128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 3923, 374, 279, 6864, 315, 9822, 30, 128009, 128006, 78191, 128007, 271, 791, 6864, 315, 9822, 374, 12366, 13]\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'llama3.2:1b'\n",
    "PROMPT = 'What is the capital of France?'\n",
    "\n",
    "res = ollama.generate(model=MODEL, prompt=PROMPT)\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separação dos parâmetros para a resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      " [INFO] Total Duration: 5.54 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{res['response']}\")\n",
    "print(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      " [INFO] Total Duration: 5.54 seconds\n"
     ]
    }
   ],
   "source": [
    "PROMPT_1 = 'What is the capital of France?'\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=[\n",
    "{'role': 'user','content': PROMPT_1,},])\n",
    "resp_1 = response['message']['content']\n",
    "print(f\"\\n{resp_1}\")\n",
    "print(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ollama.generate() = \"Limpa\" a \"memória\" do modelo após o uso\n",
    "##### Caso queira considerar a resposta anterior é usado ollama.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      " [INFO] Total Duration: 0.62 seconds\n",
      "\n",
      "The capital of Italy is Rome.\n",
      "\n",
      " [INFO] Total Duration: 2.04 seconds\n"
     ]
    }
   ],
   "source": [
    "PROMPT_1 = 'What is the capital of France?'\n",
    "response = ollama.chat(model=MODEL, messages=[\n",
    "{'role': 'user','content': PROMPT_1,},])\n",
    "resp_1 = response['message']['content']\n",
    "print(f\"\\n{resp_1}\")\n",
    "print(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n",
    "\n",
    "PROMPT_2 = 'and of Italy?'\n",
    "response = ollama.chat(model=MODEL, messages=[\n",
    "{'role': 'user','content': PROMPT_1,},\n",
    "{'role': 'assistant','content': resp_1,},\n",
    "{'role': 'user','content': PROMPT_2,},])\n",
    "resp_2 = response['message']['content']\n",
    "print(f\"\\n{resp_2}\")\n",
    "print(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
